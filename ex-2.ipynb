{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('pytorch_deeplearning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2bcd7ec7ba57b04cf90fee3838748171677195bfc714753350d4adb3113cc5c7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, docs_1, docs_2, labels, tokenizer, max_len=64):\n",
    "        self.docs_1 = docs_1\n",
    "        self.docs_2 = docs_2\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs_1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        docs_1 = str(self.docs_1[item])\n",
    "        docs_2 = str(self.docs_2[item])\n",
    "        labels = int(self.labels[item])\n",
    "        encoded_docs_1 = tokenizer.encode_plus(\n",
    "            docs_1,\n",
    "            add_special_tokens = False,\n",
    "            return_attention_mask = False,\n",
    "            return_token_type_ids = False,\n",
    "            max_length = self.max_len,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        encoded_docs_2 = tokenizer.encode_plus(\n",
    "            docs_2,\n",
    "            add_special_tokens = False,\n",
    "            return_attention_mask = False,\n",
    "            return_token_type_ids = False,\n",
    "            max_length = self.max_len,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        return dict(\n",
    "            docs_1 = docs_1, \n",
    "            docs_2 = docs_2, \n",
    "            labels = labels, \n",
    "            input_ids_1 = encoded_docs_1['input_ids'].flatten(),\n",
    "            input_ids_2 = encoded_docs_2['input_ids'].flatten()\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = CustomDataset(\n",
    "    data_train.question1.to_numpy(),\n",
    "    data_train.question2.to_numpy(), \n",
    "    data_train.is_duplicate.to_numpy(), \n",
    "    tokenizer\n",
    "    )\n",
    "validation_encoded = CustomDataset(\n",
    "    data_val.question1.to_numpy(),\n",
    "    data_val.question2.to_numpy(), \n",
    "    data_val.is_duplicate.to_numpy(), \n",
    "    tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_encoded, batch_size = 64)\n",
    "validation_loader = data.DataLoader(validation_encoded, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "India: What are job options and future options for low CGPA or graduation percentage engineering students in India (5-6)?\nJobs and Careers in India: I am currently in my third year of engineering and I have a CGPA of 7.6 with one backlog (after 4 semesters). I have decided that I want to seek a job in a firm like Mu Sigma or do MBA. Which option is better and what are the companies that I can apply for in order to get a decent job?\ntensor(0)\ntensor([ 1726,   131,  1327,  1132,  2261,  6665,  1105,  2174,  6665,  1111,\n         1822,   140, 17095,  1592,  1137,  7477,  6556,  3752,  1651,  1107,\n         1726,   113,   126,   118,   127,   114,   136,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\ntensor([18235,  1116,  1105, 17062,  1116,  1107,  1726,   131,   146,  1821,\n         1971,  1107,  1139,  1503,  1214,  1104,  3752,  1105,   146,  1138,\n          170,   140, 17095,  1592,  1104,   128,   119,   127,  1114,  1141,\n         1171, 13791,   113,  1170,   125, 14594,  1116,   114,   119,   146,\n         1138,  1879,  1115,   146,  1328,  1106,  5622,   170,  2261,  1107,\n          170,  3016,  1176, 19569, 15015,  1137,  1202, 16079,   119,  5979,\n         5146,  1110,  1618,  1105])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data['docs_1'][0])\n",
    "    print(data['docs_2'][0])\n",
    "    print(data['labels'][0])\n",
    "    print(data['input_ids_1'][0])\n",
    "    print(data['input_ids_2'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "How do I play Pokémon GO in Korea?\nHow do I play Pokémon GO in China?\ntensor(0)\ntensor([ 1731,  1202,   146,  1505, 22926, 27157,  1107,  3577,   136,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\ntensor([ 1731,  1202,   146,  1505, 22926, 27157,  1107,  1975,   136,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for data in validation_loader:\n",
    "    print(data['docs_1'][0])\n",
    "    print(data['docs_2'][0])\n",
    "    print(data['labels'][0])\n",
    "    print(data['input_ids_1'][0])\n",
    "    print(data['input_ids_2'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "q1 itos : ['India', ':', 'What', 'are', 'job', 'options', 'and', 'future', 'options', 'for', 'low', 'C', '##GP', '##A', 'or', 'graduation', 'percentage', 'engineering', 'students', 'in', 'India', '(', '5', '-', '6', ')', '?', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n===================\nq2 itos : ['Job', '##s', 'and', 'Career', '##s', 'in', 'India', ':', 'I', 'am', 'currently', 'in', 'my', 'third', 'year', 'of', 'engineering', 'and', 'I', 'have', 'a', 'C', '##GP', '##A', 'of', '7', '.', '6', 'with', 'one', 'back', '##log', '(', 'after', '4', 'semester', '##s', ')', '.', 'I', 'have', 'decided', 'that', 'I', 'want', 'to', 'seek', 'a', 'job', 'in', 'a', 'firm', 'like', 'Mu', 'Sigma', 'or', 'do', 'MBA', '.', 'Which', 'option', 'is', 'better', 'and']\n===================\nlabel : 0\n"
     ]
    }
   ],
   "source": [
    "# itos\n",
    "batch_train = next(iter(train_loader))\n",
    "print(f\"q1 itos : {tokenizer.convert_ids_to_tokens(batch_train['input_ids_1'][0])}\")\n",
    "print('===================')\n",
    "print(f\"q2 itos : {tokenizer.convert_ids_to_tokens(batch_train['input_ids_2'][0])}\")\n",
    "print('===================')\n",
    "print(f\"label : {batch_train['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tokenizer'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-800ea33c4ef7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "data_train.tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, n_vocabs, embed_dims, n_lstm_units, n_lstm_layers, n_output_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.v = n_vocabs\n",
    "        self.e = embed_dims\n",
    "        self.u = n_lstm_units\n",
    "        self.l = n_lstm_layers\n",
    "        self.o = n_output_classes\n",
    "\n",
    "        self.embed = nn.Embedding(\n",
    "            self.v,\n",
    "            self.e\n",
    "            )\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size = self.e,\n",
    "            hidden_size = self.u,\n",
    "            num_layers = self.l,\n",
    "            batch_first = True,\n",
    "            bidirectional = True,\n",
    "            dropout = 0.5\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            self.u * 4,\n",
    "            self.o\n",
    "        )\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        h0 = torch.zeros(self.l * 2, X1.size(0), self.u).to(gpu)\n",
    "        c0 = torch.zeros(self.l * 2, X1.size(0), self.u).to(gpu)\n",
    "        out1 = self.embed(X1)\n",
    "        out2 = self.embed(X2)\n",
    "        # NxTxh, lxNxh\n",
    "        out1, _ = self.bilstm(out1, (h0, c0))\n",
    "        out2, _ = self.bilstm(out2, (h0, c0))\n",
    "        out1 = out1[:, -1, :]\n",
    "        out2 = out2[:, -1, :]\n",
    "        # concatenate out1&2\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        out = self.linear(out)\n",
    "        iout = torch.max(out, 1)[1]\n",
    "        \n",
    "        return iout, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = BiLSTM(len(vocab), 512, 512, 2, 2).to(gpu)\n",
    "criterion = nn.CrossEntropyLoss().to(gpu)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr = 0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BiLSTM(\n  (embed): Embedding(65494, 512)\n  (bilstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n  (linear): Linear(in_features=2048, out_features=2, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "epoch: 1/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0087, acc: 0.71\n",
      "validation loss: 0.0041, acc: 0.74\n",
      "epoch time: 1631.84 seconds\n",
      "validation loss decreased from inf to 0.0041, saving model...\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 2/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0072, acc: 0.78\n",
      "validation loss: 0.0040, acc: 0.75\n",
      "epoch time: 1739.33 seconds\n",
      "validation loss decreased from 0.0041 to 0.0040, saving model...\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 3/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0056, acc: 0.84\n",
      "validation loss: 0.0044, acc: 0.76\n",
      "epoch time: 1735.03 seconds\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 4/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0041, acc: 0.89\n",
      "validation loss: 0.0049, acc: 0.75\n",
      "epoch time: 1713.75 seconds\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 5/20:\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0c6c37e3918d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "accuracies  = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_loss_min = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    print('epoch: {}/{}:'.format(epoch + 1, num_epochs))   \n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_tqdm_bar = tqdm(enumerate(train_iter), total = (len(train_iter)), leave = False, position = 0, file = sys.stdout, dynamic_ncols = True)\n",
    "    val_tqdm_bar = tqdm(enumerate(valid_iter), total = (len(valid_iter)),  leave = False, position = 0, file = sys.stdout, dynamic_ncols = True)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for idx, (questions, labels) in train_tqdm_bar:\n",
    "        iout, out = model(questions[0], questions[1])\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        running_corrects += torch.sum(iout == labels)\n",
    "        train_tqdm_bar.set_description(desc = 'train   '.format(epoch + 1, num_epochs))\n",
    "        batch_idx = (idx + 1) * 64\n",
    "\n",
    "        train_tqdm_bar.set_postfix(\n",
    "            loss = running_loss.item() / batch_idx if idx + 1 < len(train_iter) else running_loss.item() / len(train_iter.dataset)\n",
    "            ,acc = running_corrects.item() / batch_idx if idx + 1 < len(train_iter) else running_corrects.item() / len(train_iter.dataset)\n",
    "            )\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_idx, (v_questions, v_labels) in val_tqdm_bar:\n",
    "            v_iout, v_out = model(v_questions[0], v_questions[1])\n",
    "            v_loss = criterion(v_out, v_labels)\n",
    "            val_running_loss += v_loss\n",
    "            val_running_corrects += torch.sum(v_iout == v_labels)\n",
    "            val_tqdm_bar.set_description('validate'.format(epoch + 1, num_epochs))\n",
    "            val_batch_idx = (val_idx + 1) * 128\n",
    "\n",
    "            val_tqdm_bar.set_postfix(\n",
    "            val_loss = val_running_loss.item() / val_batch_idx if val_idx + 1 < len(valid_iter) else val_running_loss.item() / len(valid_iter.dataset)\n",
    "            ,val_acc = val_running_corrects.item() / val_batch_idx if val_idx + 1 < len(valid_iter) else val_running_corrects.item() / len(valid_iter.dataset)\n",
    "            )\n",
    "    \n",
    "    epoch_loss = running_loss/len(train_iter.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "    epoch_accuracy = running_corrects/len(train_iter.dataset)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    val_epoch_loss = val_running_loss/len(valid_iter.dataset)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_epoch_accuracy = val_running_corrects/len(valid_iter.dataset)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    checkpoint = {\n",
    "            'epoch': epoch + 1\n",
    "            ,'state_dict': model.state_dict()\n",
    "            ,'optimizer' : optimizer.state_dict()\n",
    "            ,'val_loss_min' : val_epoch_loss\n",
    "        }\n",
    "    \n",
    "    print('training loss: {:.4f}, acc: {:.2f}'.format(epoch_loss, epoch_accuracy))\n",
    "    print('validation loss: {:.4f}, acc: {:.2f}'.format(val_epoch_loss, val_epoch_accuracy))\n",
    "    print('epoch time: {:.2f} seconds'.format(time.time() - t0))\n",
    "\n",
    "    if val_epoch_loss <= val_loss_min:\n",
    "        print('validation loss decreased from {:.4f} to {:.4f}, saving model...'.format(val_loss_min, val_epoch_loss))\n",
    "        torch.save(checkpoint, 'checkpoint/question_pairs_lowest_val_loss_epoch_{}.pth'.format(epoch + 1))\n",
    "        val_loss_min = val_epoch_loss"
   ]
  }
 ]
}