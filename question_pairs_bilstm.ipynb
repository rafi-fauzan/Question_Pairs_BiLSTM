{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('pytorch_deeplearning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2bcd7ec7ba57b04cf90fee3838748171677195bfc714753350d4adb3113cc5c7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "gpu = ('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = torchtext.data.Field(\n",
    "    sequential = True,\n",
    "    lower = True,\n",
    "    batch_first = True,\n",
    "    tokenize = 'spacy',\n",
    "    pad_first = True,\n",
    "    use_vocab = True\n",
    ")\n",
    "\n",
    "question_2 = torchtext.data.Field(\n",
    "    sequential = True,\n",
    "    lower = True,\n",
    "    batch_first = True,\n",
    "    tokenize = 'spacy',\n",
    "    pad_first = True,\n",
    "    use_vocab = True\n",
    ")\n",
    "\n",
    "label = torchtext.data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab = False,\n",
    "    is_target = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchtext.data.TabularDataset(\n",
    "    path = 'dataset/train_dataset.csv', \n",
    "    format = 'csv',\n",
    "    skip_header = True,\n",
    "    fields = [\n",
    "        ('q1', question_1),\n",
    "        ('q2', question_2),\n",
    "        ('label', label)\n",
    "        ] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['how', 'hard', 'is', 'it', 'to', 'learn', 'russian', '(', 'compared', 'to', 'japanese', ')', '?']\n['if', 'you', 'can', 'read', 'chinese', ',', 'can', 'you', 'also', 'read', 'japanese', '?']\n0\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].q1)\n",
    "print(dataset[0].q2)\n",
    "print(dataset[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = dataset.split([0.7, 0.3], random_state = random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(208968, 89558)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "65494"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "question_1.build_vocab(train_dataset)\n",
    "question_2.build_vocab(train_dataset)\n",
    "question_1.vocab.extend(question_2.vocab)\n",
    "question_2.vocab = question_1.vocab\n",
    "vocab = question_1.vocab\n",
    "len(vocab)\n",
    "# vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train_dataset, validation_dataset), \n",
    "    sort_key = lambda x: len(x.q1)+len(x.q2),\n",
    "    batch_sizes = (64,128),\n",
    "    device = gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "questions_1: tensor([[   1,    1,    1,  ...,    3,  328,    2],\n        [   1,    1,    1,  ...,   14, 1086,    2],\n        [   1,    1,    1,  ...,   85,  151,    2],\n        ...,\n        [   1,    1,    1,  ...,  714,  135,    2],\n        [   1,    1,    1,  ...,   10,   41,    2],\n        [   1,    1,    1,  ...,   69,  223,    2]], device='cuda:0'), shape: torch.Size([64, 30])\nquestions_2: tensor([[   1,    1,    1,  ...,    3,  375,    2],\n        [   1,    1,    1,  ...,   14, 1114,    2],\n        [   1,    1,    1,  ...,    9,  149,    2],\n        ...,\n        [   1,    1,    1,  ...,  708,  140,    2],\n        [   1,    1,    1,  ..., 1359, 2319,    2],\n        [   1,    1,    1,  ...,   72,  220,    2]], device='cuda:0'), shape: torch.Size([64, 36])\nlables: tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0], device='cuda:0'), shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_iter:\n",
    "    print('questions_1: {}, shape: {}'.format(inputs[0], inputs[0].shape))\n",
    "    print('questions_2: {}, shape: {}'.format(inputs[1], inputs[1].shape))\n",
    "    print('lables: {}, shape: {}'.format(labels, labels.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "questions_1: tensor([[    1,    24,    74,  2064,     2],\n        [    4,     5,     9, 43922,     2],\n        [    4,     5, 48634,  2372,     2],\n        [    4,     5,     0,  1353,     2],\n        [    4,     5,  3846,   132,     2],\n        [    1,     4,     5, 36762,     2],\n        [    1, 27714,     5,   145,     2],\n        [    1,     5, 58341,  6804,     2],\n        [    1,    68,     5,  4100,     2],\n        [    1,     4,     5, 19964,     2],\n        [    1,    15,   881,  2071,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    68,     5,  7816,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    13, 10352,   251,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     1,     4,  9729,     2],\n        [    1,     5, 59958,  2267,     2],\n        [    1,     4,     5, 52622,     2],\n        [    1,    45,     5, 21550,     2],\n        [    1,   178,     0,   167,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5,   136,     2],\n        [    1,     4,     5, 38012,     2],\n        [    1,     1,   555,  2135,     2],\n        [    1,     5,     0,  2267,     2],\n        [    1,     4,     5, 18803,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5, 59198,     2],\n        [    1,     5, 12187,   232,     2],\n        [    1,     4,     5, 33172,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5, 45219,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    45,  3343, 51315,     2],\n        [    1,    45,  9129,     0,     2],\n        [    1,     4,     5, 52356,     2],\n        [    1,    45,    13,     0,     2],\n        [    1,     5,     0,  2503,     2],\n        [    1,     4,     5, 37131,     2],\n        [    1,    11,  1963,  7888,     2],\n        [    1,     5, 35708,  2267,     2],\n        [    1,     4,     5, 47492,     2],\n        [    1,    45,    13,     0,     2],\n        [    1,     4,     5,  4686,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5, 26517,     2],\n        [   45,     5,  6835,  3453,     2],\n        [    1,     4,     5, 21516,     2],\n        [    1,     5, 35708,  2267,     2],\n        [    1,    45,  5062, 36388,     2],\n        [    1,    45,  1427,  7034,     2],\n        [    1,    45,  1427, 13917,     2],\n        [    1,     4,     5, 43082,     2],\n        [    1,    13, 59321,  1148,     2],\n        [    1,     4,     5, 55627,     2],\n        [    1,     4,     5, 42133,     2],\n        [    1,     4,     5, 33172,     2],\n        [    1,     5, 17342,   184,     2],\n        [    1,     4,     5, 11730,     2],\n        [    1,     4,     5, 10579,     2],\n        [    1,    11,   304,     0,     2],\n        [    1,    68,     5, 13289,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5, 50263,     2],\n        [    4,     5,  1195,  3131,     2],\n        [    1,    45,  1869,   240,     2],\n        [    1,     5, 38381, 10214,     2],\n        [    1,     5, 41334,   305,     2],\n        [    1,     4,     5, 11730,     2],\n        [    1,    13,  8813,  4682,     2],\n        [    1,    45,   532,     0,     2],\n        [    1,     4,     5, 19859,     2],\n        [    1,     4,     5,  9751,     2],\n        [    1,     5, 48854,  2267,     2],\n        [    1,     4,     5, 50263,     2],\n        [    1,     4,     5, 16884,     2],\n        [    1,     4,     5, 11730,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    11,     7,   296,     2],\n        [    1,    45,  3343, 41037,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    45,    70, 32755,     2],\n        [    1,     4,     5,  5039,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    45,   532,     0,     2],\n        [    1,     4,     5,  3139,     2],\n        [    1,     4,     5,  2675,     2],\n        [    1,    45,     5,  5711,     2],\n        [    1,     4,     5,  1287,     2],\n        [    1,     4,     5, 11730,     2],\n        [    1,    68,     5, 20345,     2],\n        [    1,     4,     5, 18499,     2],\n        [    1,     4,     5, 43314,     2],\n        [    1,     4,     5, 56797,     2],\n        [    1,     4,     5,  9751,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,    24,   693,   101,     2],\n        [    1,     5,     0,   305,     2],\n        [    1,     5, 58189,  1747,     2],\n        [    1,    45,  8190,  5510,     2],\n        [    1,    45,  3343, 51315,     2],\n        [    1,     4,     5, 18803,     2],\n        [    1,    19,   257,  2532,     2],\n        [    1,    45,   302, 17573,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     5, 20829,  2267,     2],\n        [    1,     4,     5, 48339,     2],\n        [    1,     4,     5, 60068,     2],\n        [    1,    45,  3787, 24373,     2],\n        [    1,     4,     5, 33172,     2],\n        [    1,    45,  9129,     0,     2],\n        [    1,     5, 14080,  1988,     2],\n        [    1,     4,     5, 38617,     2],\n        [    1,     4,     5,     0,     2],\n        [    1,     1,     1,     1,  6482],\n        [    1,     5, 42368,  2267,     2],\n        [    1,     5, 41334,   305,     2],\n        [    6,  4192,    13,    17,     2],\n        [    1,     4,     5,  5021,     2],\n        [    6,     8,  1894,  2163,     2],\n        [    1,     1,     1,     1,     2],\n        [    1,     1,     1, 19449,     2],\n        [    1,     1,     1,     1, 37989]], device='cuda:0'), shape: torch.Size([128, 5])\nquestions_2: tensor([[    1,     1,     5,    74,   137, 15658,     2],\n        [    1,     1,     1,     4,     5, 41333,     2],\n        [    1,     1,     1,     4,     5,  2368,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,  4079,     2],\n        [    1,     1,     4,     5, 22502,   336,     2],\n        [    1,     1,     1,     7,     5,   164,     2],\n        [    1,     1,     1,     4,     5,  1204,     2],\n        [    1,     1,     1,     4,     5,  3877,     2],\n        [    1,     1,     1,     5, 41179,    46,     2],\n        [    1,     1,     1,    11,   836,  2110,     2],\n        [    1,     1,     1,     4,     5, 10228,     2],\n        [    1,     1,     1,     4,     5,  6645,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5, 24206,     2],\n        [    1,     1,     1,    13, 11663,  2578,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     4,  7239,    38,   134,     2],\n        [    1,     1,     1,     5, 43345,  2578,     2],\n        [    1,     1,     1,     4,     5, 35255,     2],\n        [    1,     1,     1,     5, 28676,  1261,     2],\n        [    1,     1,     1,   190,   170,     0,     2],\n        [    1,     1,     1,     4,     5, 22633,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,   981,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     4,     5,   510,  1947,     2],\n        [    1,     1,     1,     5, 46149,  2578,     2],\n        [    1,     1,     1,     4,     5, 46702,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     5, 10309,    46,     2],\n        [    1,     1,     1,     4,     5, 31134,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5, 13043,     2],\n        [    1,     1,     1,     4,     5, 35255,     2],\n        [    1,     1,     1,    45,  3029,     0,     2],\n        [    1,     1,     1,    45,  8562,     0,     2],\n        [    1,     1,     1,     4,     5, 35255,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     5, 25674,  2335,     2],\n        [    1,     1,     1,    45,     5,     0,     2],\n        [    1,     1,     1,    11, 15644, 13913,     2],\n        [    1,     1,     1,     5, 21528,  2578,     2],\n        [    1,     1,     1,     4,     5, 14142,     2],\n        [    1,     1,     1,    84,     6,     0,     2],\n        [    1,     1,     1,     4,     5, 43496,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     1,  5997,  3155,     2],\n        [    1,     1,     1,     4,     5, 20606,     2],\n        [    1,     1,     1,     5, 15111,  2578,     2],\n        [    1,     1,     1,    45,  4068,     0,     2],\n        [    1,     1,     1,    45,  1516,  2985,     2],\n        [    1,     1,     1,    45,  1516,  6896,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,    13,   827,  1137,     2],\n        [    1,     1,     1,     4,     5,  9518,     2],\n        [    1,     1,     1,     4,     5, 26155,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     5, 19799,   121,     2],\n        [    1,     1,     1,     4,     5, 46702,     2],\n        [    1,     1,     1,     4,     5,  4012,     2],\n        [    1,     1,     1,    11,   333,  2110,     2],\n        [    1,     1,     1,     5,   276,  1419,     2],\n        [    1,     1,     1,     4,     5, 37136,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     1,  1141,  3467,     2],\n        [    1,     1,     1,    45, 40142,   229,     2],\n        [    1,     1,     1,     5, 15565,  6677,     2],\n        [    1,     1,     1,     5, 47358,   315,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,    13, 11058, 10530,     2],\n        [    1,     1,     1,    45,   511, 42150,     2],\n        [    1,     1,     1,     4,     5, 40539,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     5,     0,  2578,     2],\n        [    1,     1,     1,     4,     5, 47191,     2],\n        [    1,     1,     1,     4,     5, 24441,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5, 41283,     2],\n        [    1,     1,     1,    11,    63,   324,     2],\n        [    1,     1,     1,    45,  3029,     0,     2],\n        [    1,     1,     1,     4,     5,  1185,     2],\n        [    1,     1,     1,    45,     5,     0,     2],\n        [    1,     1,     1,     4,     5,  9055,     2],\n        [    1,     1,     1,     4,     5, 35255,     2],\n        [    1,     1,     1,    45,   511,     0,     2],\n        [    1,     1,     1,     4,   485,  3747,     2],\n        [    1,     1,     1,     4,     5,  2293,     2],\n        [    1,     1,     1,     4,     5,  8227,     2],\n        [    1,     1,     1,     4,     5,  1860,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,  1573,    12, 17873,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5, 25190,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,    15,   597,    93,     2],\n        [    1,     1,     1,     5, 47539,   315,     2],\n        [    1,     1,     1,     5,     0,  1639,     2],\n        [    1,     1,     1,    45,  1516,  2563,     2],\n        [    1,     1,     1,    45,  3029,     0,     2],\n        [    1,     1,     1,     4,     5, 31134,     2],\n        [    1,     1,     1,    19, 16069,  2035,     2],\n        [    1,     1,     1,    45,  4068, 34380,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,     5,     0,  2578,     2],\n        [    1,     1,     1,     4,     5, 24206,     2],\n        [    1,     1,     1,     4,     5,  4347,     2],\n        [    1,     1,     1,    45,  4229,  1268,     2],\n        [    1,     1,     1,     4,     5,     0,     2],\n        [    1,     1,     1,    45,  8562, 39822,     2],\n        [    1,     1,     1,     5, 17802,  4170,     2],\n        [    1,     1,     1,     4,     5, 14067,     2],\n        [    1,     1,     1,     4,     5, 22417,     2],\n        [   19,    69,  4749,     5,   670,  4749,     2],\n        [    1,     1,     1,     5,     0,  2578,     2],\n        [    1,     1,     1,     5, 47539,   315,     2],\n        [    1,     1,     1,     1,     1,     7,  3664],\n        [    1,     1,     1,     1,     4,  5123,     2],\n        [    1,     1,     1,     1,     1,     4,     2],\n        [    1,     1,     1,    45,     5, 10264,     2],\n        [    1,     1,     1,     1,     1,     1, 11750],\n        [    1,     1,     1,     1,     1,     1, 35828]], device='cuda:0'), shape: torch.Size([128, 7])\nlables: tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n        0, 0, 0, 1, 0, 0, 1, 1], device='cuda:0'), shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in valid_iter:\n",
    "    print('questions_1: {}, shape: {}'.format(inputs[0], inputs[0].shape))\n",
    "    print('questions_2: {}, shape: {}'.format(inputs[1], inputs[1].shape))\n",
    "    print('lables: {}, shape: {}'.format(labels, labels.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## itos\n",
    "# batch_train = next(iter(train_iter))\n",
    "# doc_train, label_train = batch_train\n",
    "# batch_test = next(iter(valid_iter))\n",
    "# doc_valid, label_valid = batch_test\n",
    "\n",
    "# def itos_(iterator):\n",
    "#     docs_ = []\n",
    "#     for i in range (len(iterator)):\n",
    "#         x = iterator[i].item()\n",
    "#         doc = str(question1.vocab.itos[x])\n",
    "#         docs_.append(doc)\n",
    "#     docs = ' '.join(docs_)\n",
    "#     return docs, docs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(itos_(doc_train[0][0])[0])\n",
    "# print(itos_(doc_train[1][0])[0])\n",
    "# print('label: {}'.format(label_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, n_vocabs, embed_dims, n_lstm_units, n_lstm_layers, n_output_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.v = n_vocabs\n",
    "        self.e = embed_dims\n",
    "        self.u = n_lstm_units\n",
    "        self.l = n_lstm_layers\n",
    "        self.o = n_output_classes\n",
    "\n",
    "        self.embed = nn.Embedding(\n",
    "            self.v,\n",
    "            self.e\n",
    "            )\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size = self.e,\n",
    "            hidden_size = self.u,\n",
    "            num_layers = self.l,\n",
    "            batch_first = True,\n",
    "            bidirectional = True,\n",
    "            dropout = 0.5\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            self.u * 4,\n",
    "            self.o\n",
    "        )\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        # h0 = torch.zeros(self.l * 2, X.size(0), self.u).to(gpu)\n",
    "        # c0 = torch.zeros(self.l * 2, X.size(0), self.u).to(gpu)\n",
    "\n",
    "        out1 = self.embed(X1)\n",
    "        out2 = self.embed(X2)\n",
    "        # NxTxh, lxNxh\n",
    "        out1, _ = self.bilstm(out1)\n",
    "        out2, _ = self.bilstm(out2)\n",
    "        out1 = out1[:, -1, :]\n",
    "        out2 = out2[:, -1, :]\n",
    "        # concatenate out1&2\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        out = self.linear(out)\n",
    "        iout = torch.max(out, 1)[1]\n",
    "\n",
    "        return iout, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = BiLSTM(len(vocab), 512, 512, 2, 2).to(gpu)\n",
    "criterion = nn.CrossEntropyLoss().to(gpu)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr = 0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BiLSTM(\n  (embed): Embedding(65494, 512)\n  (bilstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n  (linear): Linear(in_features=2048, out_features=2, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "epoch: 1/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0087, acc: 0.71\n",
      "validation loss: 0.0041, acc: 0.74\n",
      "epoch time: 1631.84 seconds\n",
      "validation loss decreased from inf to 0.0041, saving model...\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 2/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0072, acc: 0.78\n",
      "validation loss: 0.0040, acc: 0.75\n",
      "epoch time: 1739.33 seconds\n",
      "validation loss decreased from 0.0041 to 0.0040, saving model...\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 3/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0056, acc: 0.84\n",
      "validation loss: 0.0044, acc: 0.76\n",
      "epoch time: 1735.03 seconds\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 4/20:\n",
      "------------------------------------------------------------------------------------------\n",
      "training loss: 0.0041, acc: 0.89\n",
      "validation loss: 0.0049, acc: 0.75\n",
      "epoch time: 1713.75 seconds\n",
      "------------------------------------------------------------------------------------------\n",
      "epoch: 5/20:\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0c6c37e3918d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "accuracies  = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_loss_min = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    print('epoch: {}/{}:'.format(epoch + 1, num_epochs))   \n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_tqdm_bar = tqdm(enumerate(train_iter), total = (len(train_iter)), leave = False, position = 0, file = sys.stdout, dynamic_ncols = True)\n",
    "    val_tqdm_bar = tqdm(enumerate(valid_iter), total = (len(valid_iter)),  leave = False, position = 0, file = sys.stdout, dynamic_ncols = True)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for idx, (questions, labels) in train_tqdm_bar:\n",
    "        iout, out = model(questions[0], questions[1])\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        running_corrects += torch.sum(iout == labels)\n",
    "        train_tqdm_bar.set_description(desc = 'train   '.format(epoch + 1, num_epochs))\n",
    "        batch_idx = (idx + 1) * 64\n",
    "\n",
    "        train_tqdm_bar.set_postfix(\n",
    "            loss = running_loss.item() / batch_idx if idx + 1 < len(train_iter) else running_loss.item() / len(train_iter.dataset)\n",
    "            ,acc = running_corrects.item() / batch_idx if idx + 1 < len(train_iter) else running_corrects.item() / len(train_iter.dataset)\n",
    "            )\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_idx, (v_questions, v_labels) in val_tqdm_bar:\n",
    "            v_iout, v_out = model(v_questions[0], v_questions[1])\n",
    "            v_loss = criterion(v_out, v_labels)\n",
    "            val_running_loss += v_loss\n",
    "            val_running_corrects += torch.sum(v_iout == v_labels)\n",
    "            val_tqdm_bar.set_description('validate'.format(epoch + 1, num_epochs))\n",
    "            val_batch_idx = (val_idx + 1) * 128\n",
    "\n",
    "            val_tqdm_bar.set_postfix(\n",
    "            val_loss = val_running_loss.item() / val_batch_idx if val_idx + 1 < len(valid_iter) else val_running_loss.item() / len(valid_iter.dataset)\n",
    "            ,val_acc = val_running_corrects.item() / val_batch_idx if val_idx + 1 < len(valid_iter) else val_running_corrects.item() / len(valid_iter.dataset)\n",
    "            )\n",
    "    \n",
    "    epoch_loss = running_loss/len(train_iter.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "    epoch_accuracy = running_corrects/len(train_iter.dataset)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    val_epoch_loss = val_running_loss/len(valid_iter.dataset)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_epoch_accuracy = val_running_corrects/len(valid_iter.dataset)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    checkpoint = {\n",
    "            'epoch': epoch + 1\n",
    "            ,'state_dict': model.state_dict()\n",
    "            ,'optimizer' : optimizer.state_dict()\n",
    "            ,'val_loss_min' : val_epoch_loss\n",
    "        }\n",
    "    \n",
    "    print('training loss: {:.4f}, acc: {:.2f}'.format(epoch_loss, epoch_accuracy))\n",
    "    print('validation loss: {:.4f}, acc: {:.2f}'.format(val_epoch_loss, val_epoch_accuracy))\n",
    "    print('epoch time: {:.2f} seconds'.format(time.time() - t0))\n",
    "\n",
    "    if val_epoch_loss <= val_loss_min:\n",
    "        print('validation loss decreased from {:.4f} to {:.4f}, saving model...'.format(val_loss_min, val_epoch_loss))\n",
    "        torch.save(checkpoint, 'checkpoint/question_pairs_lowest_val_loss_epoch_{}.pth'.format(epoch + 1))\n",
    "        val_loss_min = val_epoch_loss"
   ]
  }
 ]
}